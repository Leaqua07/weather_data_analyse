{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*von Benedikt Funke*\n",
    "\n",
    "### Senden der Temperaturdaten an Kafka\n",
    "\n",
    "**Wichtig!:** Bevor dieses Notebook ausgeführt werden kann muss Kafka auf dem System laufen und das `kafka-consumer.ipynb` ausgeführt werden.\n",
    "\n",
    "Die Temperaturdaten werden nicht direkt in die Datenbank geschrieben, sondern gehen einen \"Umweg\" über Kafka. Der Grund dafür liegt darin, dass es sich bei den Temperaturdaten um eine große Anzahl an Daten handelt, die so über Kafka gepuffert werden.\n",
    "\n",
    "Rein rechnerisch ergeben sich bei 494 Wetterstationen die an 62 Tagen 24 Temperaturdaten gesendet haben 735072 Datensätze. Um zu verhindern, dass Teile der Anwendung durch Datenbankoperationen blockiert werden nutzt man Kafka um die Schreiboperationen der Daten asynchron zu gestalten. In dem hier verwendeten Beispiel handelt sich jedoch zum großen Teil um eine Veranschaulichung der Nutzung von Kafka. Alle Operationen werden auf einem System ausgeführt und es sind somit keine großen Verbesserungen gegenüber der direkten Speicherung der Daten zu erwarten.\n",
    "\n",
    "Um die gewünschten Temperaturdaten zu erhalten wird zunächst eine Liste aller `STATION_ID` erstellt, für die diese abgefragt werden sollen. Dafür wird die zuvor erstelle `stations`-Collection der MongoDB abgefragt und alle `STATION_ID`s gesucht.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from wetterdienst.dwd.observations import DWDObservationData, DWDObservationParameterSet, DWDObservationResolution\n",
    "from pymongo import MongoClient\n",
    "import csv\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "stations = client.bda.stations.find({}, {'STATION_ID':1, \"_id\":0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgend wird zunächst die `KafkaProducer`-Klasse instanziiert. Bei keiner Übergabe von Parametern wie in diesem Fall, versucht der `KafkaProducer` eine Verbindung zu Kafka auf `localhost` aufzubauen. In dem hier gezeigten Beispiel läuft Kafka innerhalb des `Confluent All-In-One-Pakets` in einem Docker-Container auf dem gleichen System wie dieses Notebook.\n",
    "\n",
    "Anschließend werden zwei Hilfsfunktionen definiert, die später über den Erfolg oder Misserfolg einer Nachricht an Kafka informieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "producer = KafkaProducer()\n",
    "\n",
    "def on_send_success(record_metadata):\n",
    "    print(record_metadata.topic)\n",
    "    print(record_metadata.offset)\n",
    "\n",
    "def on_send_error(excp):\n",
    "    log.error('I am an errback', exc_info=excp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Abschnitt werden für jede Station die Temperaturdaten abgerufen und anschließend an Kafka gesendet. Dafür wird erneut die `wetterdienst`-Bibliothek verwendet. Theoretisch ließe sich bei der Abfrage der `DWDObservationData` direkt ein Array an `station_ids` einfügen. An dieser Stelle wurde sich jedoch bewusst dagegen entschieden um diesen synchronen Prozess in mehrere kleinere aufzuteilen und die Vorteile von Kafka besser nutzen zu können.\n",
    "\n",
    "Diese Abfrage liefert als Ergebnis mehrere `pandas Dataframes`. Da bei jeder Abfrage jedoch nur eine Station abgefragt wird, handelt es sich bei dieser Abfrage immer nur um einen `Dataframe`. Dieser enthält nun Informationen über die Temperatur und die Luftfeuchtigkeit zu bestimmten Zeitpunkten an der angefragten Station. Da die Luftfeuchtigkeit wird die weitere Analyse nicht verwendet werden soll, werden alle Zeilen mit Informationen über die Luftfeuchtigkeit verworfen. Anschließend wird das verbleibende `Dataframe` in eine CSV-Datei konvertiert und in die einzelnen Zeilen aufgeteilt. Diese werden nun Zeile für Zeile an Kafka gesendet. Bei einem erfolgreichen Sendevorgang wird die `on_send_success` Funktion und bei einem Fehler die `on_send_error` Funktion aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "for station in stations:\n",
    "    station_id = station['STATION_ID']\n",
    "\n",
    "    observations = DWDObservationData(\n",
    "    station_ids=[44],\n",
    "    parameters=[DWDObservationParameterSet.TEMPERATURE_AIR],\n",
    "    resolution=DWDObservationResolution.HOURLY,\n",
    "    start_date=\"2020-07-01\",\n",
    "    end_date=\"2020-08-31\",\n",
    "    tidy_data=True,\n",
    "    humanize_column_names=True,\n",
    "    )\n",
    "\n",
    "    for df in observations.collect_data():\n",
    "        df = df[df['ELEMENT'].str.match('TEMPERATURE_AIR_200')]\n",
    "        raw = df.to_csv()\n",
    "        lines = csv.reader(raw.splitlines()[1:])\n",
    "\n",
    "        for line in lines:\n",
    "            producer.send('weather', value=bytearray(str(line).strip('[]'), encoding='utf-8')).add_callback(on_send_success).add_errback(on_send_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
